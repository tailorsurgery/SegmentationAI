{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3D U-Net for Bone Segmentation",
   "id": "1784d13cc46d6b22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "In this notebook, we will implement a 3D U-Net model for bone segmentation in volumetric medical images. We will use the 3D U-Net and Attention 3D U-Net architecture to segment bones in 3D CT scans. The dataset consists of 3D CT scans and their corresponding segmentation masks. We will preprocess the dataset, extract patches with foreground pixels, and train the model on these patches. We will also compute class-wise Intersection over Union (IoU) and Dice scores to evaluate the model's performance."
   ],
   "id": "c1d6e875ddab5b8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Let's install the required libraries and modules before we get started."
   ],
   "id": "8680a608aa7fe74f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random"
   ],
   "id": "ed61edf38cd69d0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper functions\n",
   "id": "fa7114159c2a0b26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Patch extraction function\n",
    "The `extract_patches` function extracts patches from a 3D volume with the specified patch size and stride. The function slides a window over the volume with the given stride and extracts patches with the specified size. The function returns a list of patch coordinates (z_start, z_end, y_start, y_end, x_start, x_end) for each patch."
   ],
   "id": "970dfba16f70b19a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T08:22:12.878994Z",
     "start_time": "2025-02-06T08:22:12.865695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Patch extraction function with validation\n",
    "def extract_patches(volume, patch_size=(128, 128, 128), stride=(128, 128, 128)):\n",
    "    _, d, h, w = volume.shape\n",
    "\n",
    "    pd, ph, pw = patch_size\n",
    "    sd, sh, sw = stride\n",
    "\n",
    "    patches = []\n",
    "    for z in range(0, d - pd + 1, sd):\n",
    "        for y in range(0, h - ph + 1, sh):\n",
    "            for x in range(0, w - pw + 1, sw):\n",
    "                patch = volume[:, z:z + pd, y:y + ph, x:x + pw]\n",
    "                if patch.shape == (1, pd, ph, pw):  # Ensure valid shape\n",
    "                    patches.append((z, z + pd, y, y + ph, x, x + pw))\n",
    "                else:\n",
    "                    print(f\"Skipping invalid patch with shape {patch.shape}\")\n",
    "    print(f\"*Volume Shape {volume.shape}\")\n",
    "    print(f\"*Patch Size {patch_size}\")\n",
    "    print(f\"*Stride {stride}\")\n",
    "    print(f\"*Number of Patches {len(patches)}\")\n",
    "    return patches\n"
   ],
   "id": "983f001729239958",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Custom collate function for DataLoader\n",
    "The custom collate function pads the input images and masks to the maximum dimensions in the batch. This approach ensures that all images and masks have the same dimensions, allowing us to create a batch of tensors for training the model."
   ],
   "id": "c6daef64f42ed0d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Custom collate function for DataLoader\n",
    "def custom_collate(batch):\n",
    "    images, masks = zip(*batch)\n",
    "\n",
    "    # Find maximum dimensions in the batch\n",
    "    max_depth = max(img.shape[1] for img in images)\n",
    "    max_height = max(img.shape[2] for img in images)\n",
    "    max_width = max(img.shape[3] for img in images)\n",
    "\n",
    "    # Create padded tensors\n",
    "    padded_images = torch.zeros((len(images), 1, max_depth, max_height, max_width), dtype=torch.float32)\n",
    "    padded_masks = torch.zeros((len(masks), max_depth, max_height, max_width), dtype=torch.long)\n",
    "\n",
    "    for i, (img, mask) in enumerate(zip(images, masks)):\n",
    "        padded_images[i, :, :img.shape[1], :img.shape[2], :img.shape[3]] = img\n",
    "        padded_masks[i, :mask.shape[0], :mask.shape[1], :mask.shape[2]] = mask\n",
    "\n",
    "    return padded_images, padded_masks\n"
   ],
   "id": "1c2db0abd9d9ed17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### GPU usage monitoring\n",
    "This function helps us monitor the GPU usage when loading a patch in the training process."
   ],
   "id": "3d7b8935a389fea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GPU usage monitoring function\n",
    "def log_gpu_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024 ** 2\n",
    "        reserved = torch.cuda.memory_reserved() / 1024 ** 2\n",
    "        print(f\"GPU Memory: Allocated={allocated:.2f} MB, Reserved={reserved:.2f} MB\")\n",
    "        "
   ],
   "id": "3310a8c3400a6efa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataset class for patch-based training\n",
    "The `PatchBasedDataset` class reads 3D volumes and their corresponding segmentation masks, extracts patches with foreground pixels, and stores the patch coordinates. During training, the dataset class returns a random patch from the list of valid patch coordinates. This approach helps to train the model on patches with foreground pixels, reducing the number of background patches and improving the model's segmentation performance."
   ],
   "id": "1016607c643c7055"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dataset class with error handling and debug statements\n",
    "class PatchBasedDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, patch_size=(128, 128, 128), stride=(128, 128, 128), num_cases=None,\n",
    "                 visualize=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.nrrd')])\n",
    "        self.mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith('.nrrd')])\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.visualize = visualize\n",
    "\n",
    "        print(f\"Found {len(self.image_files)} images and {len(self.mask_files)} masks.\")\n",
    "        assert len(self.image_files) == len(self.mask_files), \"Number of images and masks must match!\"\n",
    "\n",
    "        if num_cases is not None:\n",
    "            selected_indices = random.sample(range(len(self.image_files)), min(num_cases, len(self.image_files)))\n",
    "            self.image_files = [self.image_files[i] for i in selected_indices]\n",
    "            self.mask_files = [self.mask_files[i] for i in selected_indices]\n",
    "\n",
    "        # Precompute patch indices for each image\n",
    "        self.patch_indices = []\n",
    "        for img_file, msk_file in zip(self.image_files, self.mask_files):\n",
    "            image_path = os.path.join(self.image_dir, img_file)\n",
    "            mask_path = os.path.join(self.mask_dir, msk_file)\n",
    "\n",
    "            # Read them with SimpleITK\n",
    "            image_sitk = sitk.ReadImage(image_path)\n",
    "            mask_sitk = sitk.ReadImage(mask_path)\n",
    "\n",
    "            image_array = sitk.GetArrayFromImage(image_sitk).astype(np.float32)\n",
    "            mask_array = sitk.GetArrayFromImage(mask_sitk).astype(np.int64)\n",
    "\n",
    "            # Normalization for the image\n",
    "            min_val, max_val = image_array.min(), image_array.max()\n",
    "            if max_val > min_val:\n",
    "                image_array = (image_array - min_val) / (max_val - min_val)\n",
    "\n",
    "            # Add channel dimension: (1, D, H, W)\n",
    "            image_array = image_array[np.newaxis, ...]\n",
    "\n",
    "            # Extract patch coords for the image\n",
    "            coords_list = extract_patches(\n",
    "                volume=image_array,\n",
    "                patch_size=self.patch_size,\n",
    "                stride=self.stride\n",
    "            )\n",
    "\n",
    "            valid_coords = []\n",
    "            for (z_start, z_end, y_start, y_end, x_start, x_end) in coords_list:\n",
    "                # Slice the mask (which is shape (D,H,W)) to see if it's all zeros\n",
    "                mask_patch = mask_array[z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "\n",
    "                if mask_patch.sum() == 0:\n",
    "                    # skip if mask patch has no foreground\n",
    "                    continue\n",
    "\n",
    "                # keep this coordinate\n",
    "                valid_coords.append((z_start, z_end, y_start, y_end, x_start, x_end))\n",
    "\n",
    "            # Now we store only the coords that have some foreground\n",
    "            self.patch_indices.append(valid_coords)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(patches) for patches in self.patch_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cumulative_patches = 0\n",
    "        for img_idx, patches in enumerate(self.patch_indices):\n",
    "            if idx < cumulative_patches + len(patches):\n",
    "                patch_idx = idx - cumulative_patches\n",
    "                patch_coords = patches[patch_idx]\n",
    "                break\n",
    "            cumulative_patches += len(patches)\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[img_idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[img_idx])\n",
    "\n",
    "        image = sitk.ReadImage(image_path)\n",
    "        mask = sitk.ReadImage(mask_path)\n",
    "\n",
    "        image_array = sitk.GetArrayFromImage(image).astype(np.float32)\n",
    "        mask_array = sitk.GetArrayFromImage(mask).astype(np.int64)\n",
    "\n",
    "        image_array = (image_array - np.min(image_array)) / (np.max(image_array) - np.min(image_array))\n",
    "        image_array = image_array[np.newaxis, ...]\n",
    "\n",
    "        z_start, z_end, y_start, y_end, x_start, x_end = patch_coords\n",
    "        image_patch = image_array[:, z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "        mask_patch = mask_array[z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "\n",
    "        image_tensor = torch.tensor(image_patch, dtype=torch.float32)\n",
    "        mask_tensor = torch.tensor(mask_patch, dtype=torch.long)\n",
    "\n",
    "        return image_tensor, mask_tensor"
   ],
   "id": "1263c41bb633e9a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model architecture",
   "id": "c763c30dbdcc9a22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3D U-Net model\n",
    "The 3D U-Net model is a popular architecture for volumetric segmentation tasks. It consists of an encoder-decoder structure with skip connections between corresponding layers in the encoder and decoder paths. The skip connections help propagate high-resolution features from the encoder to the decoder, improving the segmentation performance. The model takes a 3D volume as input and outputs a segmentation mask with the same dimensions."
   ],
   "id": "74a859bedf99ddcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3D U-Net model\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3D, self).__init__()\n",
    "\n",
    "        # Encoder blocks\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv3d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Up-convolution blocks\n",
    "        self.upconv3 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Conv3d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Conv3d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Conv3d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Final output layer\n",
    "        self.final_conv = nn.Conv3d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path with skip connections\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "\n",
    "        # Middle block\n",
    "        mid = self.middle(self.pool(enc3))\n",
    "\n",
    "        # Decoder path\n",
    "        dec3 = self.upconv3(mid)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        return self.final_conv(dec1)"
   ],
   "id": "8583c86ee41c19ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Attention 3D U-Net\n",
    "The Attention 3D U-Net model is an extension of the 3D U-Net model with attention gates. The attention mechanism helps the model focus on relevant regions in the input data, improving the segmentation performance. The attention block takes two inputs: the gating signal (from the decoder) and the skip connection (from the encoder). The attention block combines these inputs to produce an alpha mask that is multiplied with the skip connection to refine the features."
   ],
   "id": "248c2a00b0bf42d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Attention Block\n",
    "class AttentionBlock3D(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3D Attention Gate as described in 'Attention U-Net: Learning Where to Look for the Pancreas'\n",
    "    (https://arxiv.org/abs/1804.03999).\n",
    "\n",
    "    Typically:\n",
    "      - g: gating signal (from decoder)\n",
    "      - x: features from encoder (skip connection)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels_g, in_channels_x, inter_channels):\n",
    "        super(AttentionBlock3D, self).__init__()\n",
    "\n",
    "        # W_g: transform gating signal\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv3d(in_channels_g, inter_channels, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm3d(inter_channels)\n",
    "        )\n",
    "\n",
    "        # W_x: transform skip connection\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv3d(in_channels_x, inter_channels, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm3d(inter_channels)\n",
    "        )\n",
    "\n",
    "        # psi: combines the two transforms and outputs alpha mask\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv3d(inter_channels, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm3d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # ReLU for gating\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        # g: gating (decoder)\n",
    "        # x: skip connection (encoder)\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "\n",
    "        # resample gating if needed (e.g., if shapes differ)\n",
    "        # But typically you ensure shapes match by design (e.g., with MaxPool / UpConv).\n",
    "        # For example, if x is bigger, you might do a resize or interpolation here.\n",
    "        # g1 = F.interpolate(g1, size=x1.shape[2:], mode='trilinear', align_corners=False)\n",
    "\n",
    "        # combine\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)      # shape: (N,1,D,H,W)\n",
    "\n",
    "        # Multiply skip connection by attention map\n",
    "        out = x * psi\n",
    "        return out\n",
    "\n",
    "class UNet3D_Attention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3D_Attention, self).__init__()\n",
    "\n",
    "        # --- [Encoder blocks] ---\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # --- [Middle block] ---\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv3d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.att3 = AttentionBlock3D(in_channels_g=256, in_channels_x=256, inter_channels=256)\n",
    "        self.att2 = AttentionBlock3D(in_channels_g=128, in_channels_x=128, inter_channels=64)\n",
    "        self.att1 = AttentionBlock3D(in_channels_g=64, in_channels_x=64, inter_channels=32)\n",
    "\n",
    "        # --- [Decoder blocks with up-convs] ---\n",
    "        self.upconv3 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Conv3d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Conv3d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Conv3d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # --- [Final output layer] ---\n",
    "        self.final_conv = nn.Conv3d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Encoder path ---\n",
    "        enc1 = self.encoder1(x)              # shape: (64, D, H, W)\n",
    "        enc2 = self.encoder2(self.pool(enc1)) # shape: (128, D/2, H/2, W/2)\n",
    "        enc3 = self.encoder3(self.pool(enc2)) # shape: (256, D/4, H/4, W/4)\n",
    "\n",
    "        # --- Middle ---\n",
    "        mid = self.middle(self.pool(enc3))    # shape: (512, D/8, H/8, W/8)\n",
    "\n",
    "        # --- Decoder path with Attention ---\n",
    "\n",
    "        # 1) Decoder block at level 3\n",
    "        dec3 = self.upconv3(mid)  # upsample from 512 -> 256\n",
    "        # apply attention gate to enc3\n",
    "        enc3_att = self.att3(g=dec3, x=enc3)  # returns refined skip\n",
    "        # concat\n",
    "        dec3 = torch.cat([dec3, enc3_att], dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        # 2) Decoder block at level 2\n",
    "        dec2 = self.upconv2(dec3) # upsample from 256 -> 128\n",
    "        enc2_att = self.att2(g=dec2, x=enc2)\n",
    "        dec2 = torch.cat([dec2, enc2_att], dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        # 3) Decoder block at level 1\n",
    "        dec1 = self.upconv1(dec2) # upsample from 128 -> 64\n",
    "        enc1_att = self.att1(g=dec1, x=enc1)\n",
    "        dec1 = torch.cat([dec1, enc1_att], dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        # --- Final ---\n",
    "        output = self.final_conv(dec1)\n",
    "        return output\n",
    "    "
   ],
   "id": "cf9ecb5cf4bfaa04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training the 3D U-Net model\n",
    "\n",
    "The training function trains the 3D U-Net model with the given training and validation data loaders. The function also computes the class-wise Intersection over Union (IoU) and Dice scores for each class in the binary segmentation task. The model is saved after each epoch.\n"
   ],
   "id": "4204bed0b50de557"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training function with IoU and Dice\n",
    "def train_model(model, train_loader, val_loader, device, optimizer, epochs=20, lr=1e-3,\n",
    "                model_save_path=\"with_attention\"):\n",
    "    print(\"********************* Starting training *********************\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Example: Weighted cross-entropy\n",
    "    class_weights = torch.tensor([0.1, 1.0]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    CLASS_LIST = list(range(2))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # -----------------------------\n",
    "        # TRAINING LOOP\n",
    "        # -----------------------------\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\")):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Log GPU usage periodically\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                log_gpu_usage()\n",
    "\n",
    "            del images, masks, outputs, loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # VALIDATION LOOP\n",
    "        # -----------------------------\n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            # Accumulators for classwise IoU & Dice across entire val set\n",
    "            class_intersections = {cls: 0 for cls in CLASS_LIST}\n",
    "            class_unions = {cls: 0 for cls in CLASS_LIST}\n",
    "            class_predsums = {cls: 0 for cls in CLASS_LIST}\n",
    "            class_targetsums = {cls: 0 for cls in CLASS_LIST}\n",
    "            slice_saved = False\n",
    "            with torch.no_grad():\n",
    "                for images, masks in tqdm(val_loader, desc=\"Validation\"):\n",
    "                    images, masks = images.to(device), masks.to(device)\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, masks)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # Predicted labels\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    SAVE_DIR = \"./saved_validation_slices\"\n",
    "                    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "                    # Save a slice\n",
    "                    if not slice_saved:\n",
    "                        for i in range(images.size(0)):  # Iterate over the batch\n",
    "                            slice_idx = images.size(2) + 12 // 2  # Select the middle slice\n",
    "                            if slice_idx < images.size(2):  # Ensure slice index is valid\n",
    "                                # Extract slices\n",
    "                                input_slice = images[i, 0, slice_idx, :, :].cpu().numpy()\n",
    "                                gt_slice = masks[i, slice_idx, :, :].cpu().numpy()\n",
    "                                pred_slice = preds[i, slice_idx, :, :].cpu().numpy()\n",
    "\n",
    "                                # Normalize slices to [0, 255]\n",
    "                                def normalize_to_uint8(slice_data):\n",
    "                                    return ((slice_data - np.min(slice_data)) / (\n",
    "                                            np.max(slice_data) - np.min(slice_data)) * 255).astype(np.uint8)\n",
    "\n",
    "                                input_slice = normalize_to_uint8(input_slice)\n",
    "                                gt_slice = normalize_to_uint8(gt_slice)\n",
    "                                pred_slice = normalize_to_uint8(pred_slice)\n",
    "\n",
    "                                # Save slices\n",
    "                                Image.fromarray(input_slice).save(\n",
    "                                    os.path.join(SAVE_DIR, f\"epoch_{epoch + 1}_input_slice.png\"))\n",
    "                                Image.fromarray(gt_slice).save(\n",
    "                                    os.path.join(SAVE_DIR, f\"epoch_{epoch + 1}_ground_truth_slice.png\"))\n",
    "                                Image.fromarray(pred_slice).save(\n",
    "                                    os.path.join(SAVE_DIR, f\"epoch_{epoch + 1}_prediction_slice.png\"))\n",
    "\n",
    "                                print(f\"Saved validation slices for epoch {epoch + 1}.\")\n",
    "                                slice_saved = True  # Save only once per validation phase\n",
    "                                break\n",
    "\n",
    "                    # Accumulate stats for each class\n",
    "                    # Flatten so we can sum easily (N, D,H,W) -> all voxels\n",
    "                    preds_flat = preds.view(-1)\n",
    "                    masks_flat = masks.view(-1)\n",
    "\n",
    "                    for cls in CLASS_LIST:\n",
    "                        pred_mask = (preds_flat == cls)\n",
    "                        tgt_mask = (masks_flat == cls)\n",
    "\n",
    "                        inter = (pred_mask & tgt_mask).sum().item()\n",
    "                        uni = (pred_mask | tgt_mask).sum().item()\n",
    "\n",
    "                        class_intersections[cls] += inter\n",
    "                        class_unions[cls] += uni\n",
    "                        class_predsums[cls] += pred_mask.sum().item()\n",
    "                        class_targetsums[cls] += tgt_mask.sum().item()\n",
    "\n",
    "                    del images, masks, outputs, preds\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            # -----------------------------\n",
    "            # COMPUTE CLASS-WISE METRICS\n",
    "            # -----------------------------\n",
    "            class_ious = {}\n",
    "            class_dices = {}\n",
    "            for cls in CLASS_LIST:\n",
    "                intersection = class_intersections[cls]\n",
    "                union = class_unions[cls]\n",
    "                pred_sum = class_predsums[cls]\n",
    "                tgt_sum = class_targetsums[cls]\n",
    "\n",
    "                # IoU\n",
    "                iou_val = intersection / union if union > 0 else 0.0\n",
    "                # Dice\n",
    "                dice_val = (2.0 * intersection) / (pred_sum + tgt_sum) if (pred_sum + tgt_sum) > 0 else 0.0\n",
    "\n",
    "                class_ious[cls] = iou_val\n",
    "                class_dices[cls] = dice_val\n",
    "\n",
    "            # Mean IoU & Dice across classes\n",
    "            mean_iou = np.mean(list(class_ious.values()))\n",
    "            mean_dice = np.mean(list(class_dices.values()))\n",
    "\n",
    "            # Print results\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\\n\")\n",
    "\n",
    "            print(\"Class-wise IoU:\")\n",
    "            for cls in CLASS_LIST:\n",
    "                print(f\"  - Class {cls}: {class_ious[cls]:.4f}\")\n",
    "            print(f\"Mean IoU: {mean_iou:.4f}\\n\")\n",
    "\n",
    "            print(\"Class-wise Dice:\")\n",
    "            for cls in CLASS_LIST:\n",
    "                print(f\"  - Class {cls}: {class_dices[cls]:.4f}\")\n",
    "            print(f\"Mean Dice: {mean_dice:.4f}\\n\")\n",
    "\n",
    "        # Save model each epoch\n",
    "        torch.save(model.state_dict(), f\"{model_save_path}_epoch{epoch + 1}.pth\")\n",
    "        print(f\"Model saved to {model_save_path}_epoch{epoch + 1}.pth\")\n",
    "\n",
    "    print(f\"Training completed in {time.time() - start_time:.2f} seconds.\")\n"
   ],
   "id": "55fc6fbb8dca75e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main program",
   "id": "42d1fddf175bdebf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Main program to train the 3D U-Net model on the binary bone segmentation task.\n",
    "Make sure to change the `region` variable to `binary` or `multiclass` based on the dataset you are using and also the model_type you want to use weather it's `basic` or `attention`. The script will preprocess the dataset, split it into training and validation sets, and train the 3D U-Net model on the patches extracted from the dataset. The model will be saved between epochs and after training."
   ],
   "id": "3cfd5cc21f9dce88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_type = \"attention\"\n",
    "    \n",
    "    region = 'arms/binary'\n",
    "    script_dir = \"\"\n",
    "    image_dir = f\"{script_dir}/data/segmentai_dataset/images/{region}\"\n",
    "    mask_dir = f\"{script_dir}/data/segmentai_dataset/multiclass_masks/{region}\"\n",
    "    dataset_dir = f\"{script_dir}/data/segmentai_dataset/processed/{region}_dataset.pth\"\n",
    "    model_save_path = f\"{script_dir}/models/unet/{region}\"\n",
    "\n",
    "    # Load dataset\n",
    "    if os.path.exists(dataset_dir):\n",
    "        print(\"Loading preprocessed dataset...\")\n",
    "        full_dataset = torch.load(dataset_dir)\n",
    "    else:\n",
    "        if not os.path.exists(image_dir) or not os.path.exists(mask_dir):\n",
    "            print(\"Image dir: \", image_dir)\n",
    "            print(\"Mask dir: \", mask_dir)\n",
    "            raise FileNotFoundError(\"Image or mask directory not found!\")\n",
    "        full_dataset = PatchBasedDataset(image_dir, mask_dir, visualize=False)\n",
    "        torch.save(full_dataset, dataset_dir)\n",
    "\n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=1, pin_memory=True,\n",
    "                              collate_fn=custom_collate)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=1, pin_memory=True,\n",
    "                            collate_fn=custom_collate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if model_type == \"basic\":\n",
    "        model = UNet3D(1, 2).to(device)\n",
    "    elif model_type == \"attention\":\n",
    "        model = UNet3D_Attention(1, 2).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training\n",
    "    train_model(model, train_loader, val_loader, device, optimizer, epochs=50, lr=1e-3)\n",
    "    torch.save(model.state_dict(), f\"{model_save_path}_attention_final.pth\")\n",
    "    print(f\"Model saved to {model_save_path}_attention_final.pth\")\n"
   ],
   "id": "ff35c284c85e8af5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
